<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fixed Copy to Clipboard</title>
</head>
<body>
    <textarea id="text1" readonly>WSL Installation and Hadoop Setup:
wsl --install or wsl --install -d ubuntu
Username = venom, Password = venom
sudo apt-get update
sudo apt-get install openjdk-8-jdk
java -version
sudo addgroup patkar09
sudo adduser --ingroup patkar09 venom16
sudo usermod -aG sudo venom16 
su - venom16
ssh-keygen -t rsa -P “”(Generating public/private rsa key pair.) 
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys 
sudo nano /etc/sysctl.conf 
net.ipv6.conf.all.disable_ipv6 = 1  
net.ipv6.conf.default.disable_ipv6 = 1 
net.ipv6.conf.lo.disable_ipv6 = 1 
Save and exit (Ctrl + X, then Y and Enter).

Now we will download Hadoop. 
cd /usr/local 
sudo wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz
sudo tar xzf hadoop-3.3.1.tar.gz 
sudo mv hadoop-3.3.1 hadoop 
sudo chown -R venom16:patkar456 hadoop 
nano $HOME/.bashrc 
end of the file: #Hadoop Related Options 
export HADOOP_HOME=/usr/local/hadoop 
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64 
export HADOOP_INSTALL=$HADOOP_HOME 
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib” 
Save and exit (Ctrl + X, then Y and Enter).

source ~/.bashrc
cd /usr/local/hadoop/etc/hadoop  

Add the following line to hadoop-env.sh 
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64 
Save and exit (Ctrl + X, then Y and Enter).

nano hadoop-env.sh 
sudo mkdir -p /app/hadoop/tmp 
sudo chown venom16:patkar456 /app/hadoop/tmp 
sudo chown venom16:patkar456 /app/hadoop/tmp 
nano core-site.xml 
Add the following between 
<configuration> and </configuration> 
<property> 
<name>hadoop.tmp.dir</name> 
<value>/app/hadoop/tmp</value> 
</property> 
<property> 
<name>fs.default.name</name> 
<value>hdfs://localhost:54310</value> 
</property> 
Save and exit (Ctrl + X, then Y and Enter).

nano mapred-site.sh 
Add the following between 
<configuration> and </configuration> 
<property> 
<name>mapred.job.tracker</name> 
<value>localhost:54311</value> 
</property> 
 
nano hdfs-site.xml 
<property> 
<name>dfs.replication</name> 
<value>1</value> 
</property> 

hadoop namenode -format 
ssh localhost 
sudo apt-get install ssh 
sudo service ssh restart
ssh localhost 
cd /usr/local/hadoop/sbin 
start-all.sh 
jps
stop-all.sh 
</textarea>
    <button onclick="copyToClipboard('text1', this)">Hadoop Setup</button>

    <textarea id="text2" readonly>Java & Hadoop MapReduce Setup:
    
java --version
hadoop version
su venom16

Create a Text File with Sample Words:
Hello wordcount MapReduce Hadoop program.
This is my first MapReduce program

sudo nano Mapreduce.txt
Write this:
Hello wordcount MapReduce Hadoop program.
This is my first MapReduce program
Save and exit (Ctrl + X, then Y and Enter).

cat Mapreduce.txt
start-all.sh
hdfs dfs -put /home/venom16/Mapreduce.txt /
nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh

Add these lines: and save and exit
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"

source ~/.bashrc
hdfs dfs -put /home/venom16/Mapreduce.txt /
cd /usr/local/hadoop/share/hadoop/mapreduce 
hadoop jar hadoop-mapreduce-examples-3.3.1.jar wordcount /Mapreduce.txt /output 
hdfs dfs -ls /output 
hdfs dfs -cat /Mapreduce.txt /output/part-r-00000  
hdfs dfs -head /output/part-r-00000 
hdfs dfs -mv /output/part-r-00000 /output/outpt.txt 
hdfs dfs -ls /output 
hdfs dfs -get /output/outpt.txt /home/venom16 
ls
cat output.txt
hdfs dfs -ls 
hdfs dfs -cat /output/outpt.txt 
hdfs dfs -rm /Mapreduce.txt 
hdfs dfs -rm -r /output</textarea>
    <button onclick="copyToClipboard('text2', this)">MapReduce</button>

<textarea id="text3" readonly>MongoDB Setup & Operations:
    
mongod --version
python -m pip install pymongo

Creating a Database, Collection and inserting one collection:
import pymongo 
client = pymongo.MongoClient("mongodb://localhost:27017/") 
print(client) 
mydb = client["BigData"] 
print(client.list_database_names()) 
collection = mydb["student"] 
dictionary = {'name' : 'Roshni' , 'dept' : 'ITCS'} 
collection.insert_one(dictionary) 
print(mydb.list_collection_names()) 
InMongoShell: show dbs, use BigData, db. student.find()

Inserting more collections:
import pymongo
client = pymongo.MongoClient("mongodb://localhost:27017/")
print(client)
mydb = client["BigData"]
print(client.list_database_names())
collection = mydb["student"]
mylist= [{'name' : 'Omkar' , 'dept' : 'Arts'},
 {'name' : 'Vedant' , 'dept' : 'BMM'},
 {'name' : 'Rohit', 'dept' : 'BMS'},
 {'name' : 'Amey' , 'dept' : 'Commerce'},]

collection.insert_many(mylist)
print(mydb.list_collection_names())
Mongoshell: db.student.find().pretty()

To show all collections:
import pymongo
client = pymongo.MongoClient("mongodb://localhost:27017/")
print(client)
mydb = client["BigData"]
print(client.list_database_names())
collection = mydb["student"]
alldocs=collection.find()
for item in alldocs:
 print(item)

To update one collection:
import pymongo
client = pymongo.MongoClient("mongodb://localhost:27017/")
print(client)
mydb = client["BigData"]
print(client.list_database_names())
collection = mydb["student"]
collection.update_one({'name': 'Amey'},
 {'$set' : {'dept' : 'Data Science'}})
alldocs= collection.find()
for item in alldocs:
 print(item)

To delete one collection:
import pymongo
client = pymongo.MongoClient("mongodb://localhost:27017/")
print(client)
mydb = client["BigData"]
print(client.list_database_names())
collection = mydb["student"]
collection.delete_one({'name': 'Amey'})

alldocs= collection.find()
for item in alldocs:
 print(item)</textarea>
    <button onclick="copyToClipboard('text3', this)">MongoDB</button>

<textarea id="text4" readonly>Pig Setup & Processing:
    
java --version
su venom16
sudo service ssh restart 
ssh localhost 
start-all.sh 
cd /home/venom16 
sudo nano sample.txt 

Add following details in the sample.txt: (comma separated values)  
1,Rasika,20000,Lecturer  
2,Hrishi,25000,Accountant  
3,Rahul,30000,SoftwareEngineer 
Save “Ctrl S” and exit the file by pressing “Ctrl X 

cd /usr/local 
sudo wget https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz 
sudo tar -xvzf pig-0.17.0.tar.gz
ls
sudo mv pig-0.17.0 pig 
ls -l 
sudo chmod 777 pig 

cd /home/hduser1 
nano .bashrc 
Append the following Pig environment variables:
export PATH=$PATH:$HADOOP_HOME/bin  
export PATH=$PATH:$HADOOP_HOME/sbin  
export PATH=$PATH:/usr/local/pig/bin  
export PIG_HOME=/usr/local/pig  
export PIG_CLASSPATH=/usr/local/hadoop/etc/hadoop  
save and exit, then source .bashrc  
pig -version  
pig -x local
quit
pig
pig -x local
Employees = LOAD ‘sample.txt’ USING PigStorage(‘,’) as (id:int, firstname:chararray,salary:int, Dept:chararray);  
dump Employees;</textarea>
    <button onclick="copyToClipboard('text4', this)">Pig</button>

<textarea id="text5" readonly>Hive Setup & Processing:
    
java --version
hadoop version
sudo service ssh restart  
ssh localhost  
start-all.sh 
cd /home/venom16  
sudo nano sample.txt  

Add following details in the sample.txt:
1 Rasika 20000 Lecturer  
2 Hrishi 25000 Accountant  
3 Rahul 30000 SoftwareEngineer 
Save and exit (Ctrl + X, then Y and Enter). 

cd /usr/local  
sudo wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
sudo tar -xvzf apache-hive-3.1.2-bin.tar.gz 
ls
sudo mv apache-hive-3.1.2-bin hive 
ls
sudo chmod 777 hive  
cd /home/hduser1 
nano .bashrc

Append the following Hive environment variables to the .bashrc file:  
export HIVE_HOME=/usr/local/hive  
export PATH=$PATH:$HIVE_HOME/bin  
Save and exit, then source .bashrc

Edit hive-config.sh file:
cd /usr/local/hive/bin
sudo nano hive-config.sh

Add the following line to hive-config.sh: 
export HADOOP_HOME=/usr/local/hadoop.

hdfs dfs -mkdir /tmp 
hdfs dfs -chmod g+w /tmp
hdfs dfs -ls /
hdfs dfs -mkdir -p /user/hive/warehouse  
hdfs dfs -chmod g+w /user/hive/warehouse 
hdfs dfs -ls /user/hive

/usr/local/hive/bin/schematool -initSchema -dbType derby  

To fix Guava incompatibility:
ls $HIVE_HOME/lib  
ls $HADOOP_HOME/share/hadoop/hdfs/lib
sudo rm /usr/local/hive/lib/guava-19.0.jar 
sudo cp $HADOOP_HOME/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/hive/lib/ 
/usr/local/hive/bin/schematool -initSchema -dbType derby  

cd $HIVE_HOME  
hive 
show tables

CREATE TABLE IF NOT EXISTS employee ( eid int, name String, salary String, designation String) 
COMMENT ‘Employee details’; 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ' ' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE; 

LOAD DATA LOCAL INPATH “/home/hduser1/sample.txt” OVERWRITE INTO TABLE employee; 
SELECT * FROM employee;
QUIT;</textarea>
    <button onclick="copyToClipboard('text5', this)">Hive</button>

<textarea id="text6" readonly>Decision Tree Classifier:

# Importing Required Libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier  # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split  # Import train_test_split function
from sklearn import metrics  # Import sklearn metrics module for accuracy calculation

# Loading Data
col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']
# Load dataset
pima = pd.read_csv("diabetes.csv", names=col_names, header=1)
print(pima.head())  # Display the first 5 rows

# Feature Selection
# Split dataset into features and target variable
feature_cols = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']
X = pima[feature_cols]  # Features
y = pima['label']  # Target variable

# Splitting Dataset
# Split dataset into training set and test set (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Building Decision Tree Model
# Create Decision Tree classifier object
clf = DecisionTreeClassifier()
# Train Decision Tree Classifier
clf.fit(X_train, y_train)
# Predict the response for the test dataset
y_pred = clf.predict(X_test)

# Evaluating Model
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))

# Visualizing Decision Tree
from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

# Generate decision tree visualization
dot_data = StringIO()
export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,
                feature_names=feature_cols, class_names=['0', '1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())</textarea>
    <button onclick="copyToClipboard('text6', this)">DecisionTree</button>

<textarea id="text7" readonly>SVM Classifier:

import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data[:, :2]  # Taking only the first two features (sepal length & width)
y = iris.target  # Target labels

# Splitting dataset into training and test sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Creating and training an SVM classifier with a linear kernel
clf = svm.SVC(kernel='linear', C=1)
clf.fit(x_train, y_train)

# Making predictions on the test set
classifier_predictions = clf.predict(x_test)

# Evaluating accuracy
accuracy = accuracy_score(y_test, classifier_predictions) * 100
print(f"Accuracy: {accuracy:.2f}%")

# Plotting decision boundary
h = 0.02  # Step size in the mesh
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

# Creating a mesh grid
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# Predicting for each grid point
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors="k")
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.title("SVM with Linear Kernel")

# Show the plot
plt.show()

</textarea>
    <button onclick="copyToClipboard('text7', this)">SVM</button>

<textarea id="text8" readonly>Admission Prediction using Logistic Regression:

# Importing required libraries
import numpy as np  # Linear algebra
import pandas as pd  # Data processing, CSV file I/O
import matplotlib.pyplot as plt  # Visualization

# Loading dataset
student_data = pd.read_csv("Admission_P1A.csv")

# Print first ten records
print(student_data.head(10))

# Split dataset into features and target variable
feature_cols = ['gre', 'gpa', 'rank']
X = student_data[feature_cols]
Y = student_data['admit']  # Ensure correct column access

# Splitting dataset into training (70%) and test (30%) sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)

# Model building - Logistic Regression
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(solver='liblinear')  # Added solver for compatibility

# Fit the model with training data
clf.fit(X_train, Y_train)

# Predicting on test dataset
Y_pred = clf.predict(X_test)

# Accuracy calculation and display
from sklearn import metrics
print("Accuracy:", round(metrics.accuracy_score(Y_test, Y_pred) * 100, 2), "%")  # Fixed closing parenthesis

# Prediction for new data
new = {'gre': [260], 'gpa': [2.67], 'rank': [1]}
sc2 = pd.DataFrame(new, columns=['gre', 'gpa', 'rank'])

# Predict admission for new student data
Y_pred_new = clf.predict(sc2)

print("\nNew Data:")
print(sc2)
print("Prediction:", "Admitted" if Y_pred_new[0] == 1 else "Not Admitted")

</textarea>
    <button onclick="copyToClipboard('text8', this)">LogisticRegression</button>

    <textarea id="text9" readonly>Linear Regression for Admission Prediction:

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics

# Load dataset
df = pd.read_csv("Admission_Predict.csv")

# Display basic info
print(df.head())
print(df.info())

# Clean column names
df.columns = (
    df.columns
    .str.strip()
    .str.lower()
    .str.replace(' ', '_', regex=True)
    .str.replace('(', '', regex=True)
    .str.replace(')', '', regex=True)
)

# One-hot encoding (if needed)
df_dummies = pd.get_dummies(df, drop_first=True)

# Shuffle dataset
df_shuffled = shuffle(df_dummies, random_state=44)

# Define independent (X) and dependent (y) variables
DV = 'chance_of_admit'
X = df_shuffled.drop(DV, axis=1)
y = df_shuffled[DV]

# Split into training and test sets (67% train, 33% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Check training data
print(X_train.head())
print(y_train.head())

# Train Linear Regression Model using only 'gre_score'
model = LinearRegression()
model.fit(X_train[['gre_score']], y_train)

# Display intercept and coefficient
intercept = model.intercept_
coeff = model.coef_[0]  # Extract single coefficient

print("\nIntercept:", round(intercept, 2))
print("Coefficient:", round(coeff, 2))
print(f"Chance of Admit = {intercept:.2f} + ({coeff:.2f} x GRE Score)")

# Make predictions on test data
y_pred = model.predict(X_test[['gre_score']])

# Display actual vs predicted values
df1 = pd.DataFrame({'Actual Chance of Admit': y_test, 'Predicted Chance of Admit': y_pred})
print(df1.head())

# Model evaluation
print("\nModel Evaluation Metrics:")
print("Mean Absolute Error:", round(metrics.mean_absolute_error(y_test, y_pred), 4))
print("Mean Squared Error:", round(metrics.mean_squared_error(y_test, y_pred), 4))
print("Root Mean Squared Error:", round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 4))
print("Mean Absolute Percentage Error:", round(metrics.mean_absolute_percentage_error(y_test, y_pred), 4))
print("R-squared:", round(metrics.r2_score(y_test, y_pred), 4))  # Fixed R-squared metric

</textarea>
    <button onclick="copyToClipboard('text9', this)">LinearRegression</button>

<textarea id="text10" readonly>import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Load Dataset
df = pd.read_csv('spam.csv', encoding='latin-1')

# Keep only necessary columns
df = df[['v2', 'v1']]

# Rename columns
df.columns = ['SMS', 'Type']

# Instantiate count vectorizer
countvec = CountVectorizer(
    ngram_range=(1, 4),
    stop_words='english',
    strip_accents='unicode',
    max_features=1000
)

# Create bag of words
bow = countvec.fit_transform(df['SMS'])

# Prepare training data
X_train = bow.toarray()
y_train = df['Type'].values

# Instantiate classifier
mnb = MultinomialNB()

# Train the classifier
mnb.fit(X_train, y_train)

# Testing
text = countvec.transform(['Free gifts for all'])
print(mnb.predict(text))
</textarea>
    <button onclick="copyToClipboard('text10', this)">Classifier</button>

 <textarea id="text11" readonly># Import statements
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans  # Import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Create blobs
data = make_blobs(n_samples=400, n_features=2, centers=4, cluster_std=1.6, random_state=50)

# Create np array for data points
points = data[0]

# Create scatter plot of original data
plt.scatter(points[:, 0], points[:, 1], c=data[1], cmap='viridis', alpha=0.5)
plt.xlim(-15, 15)
plt.ylim(-15, 15)
plt.title("Original Data Distribution")
plt.show()

# Apply K-Means Clustering
kmeans = KMeans(n_clusters=4, random_state=42)
y_km = kmeans.fit_predict(points)

# Visualize the Clusters
plt.scatter(points[y_km == 0, 0], points[y_km == 0, 1], s=100, c='red', label="Cluster 1")
plt.scatter(points[y_km == 1, 0], points[y_km == 1, 1], s=100, c='blue', label="Cluster 2")
plt.scatter(points[y_km == 2, 0], points[y_km == 2, 1], s=100, c='grey', label="Cluster 3")
plt.scatter(points[y_km == 3, 0], points[y_km == 3, 1], s=100, c='green', label="Cluster 4")

# Plot cluster centers
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            s=200, color='yellow', marker="*", edgecolors='black", label="Centroids")

plt.xlim(-15, 15)
plt.ylim(-15, 15)
plt.legend()
plt.title("K-Means Clustering")
plt.show()
</textarea>
    <button onclick="copyToClipboard('text11', this)">Clustering</button>
    <script>
        function copyToClipboard(elementId, button) {
            let textElement = document.getElementById(elementId);
            navigator.clipboard.writeText(textElement.value).then(() => {
                let originalText = button.innerText;
                button.innerText = "Copied!";
                setTimeout(() => {
                    button.innerText = originalText;
                }, 1500);
            }).catch(err => {
                console.error("Failed to copy: ", err);
            });
        }
    </script>
</body>
</html>
